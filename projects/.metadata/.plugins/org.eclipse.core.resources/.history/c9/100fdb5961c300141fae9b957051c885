package stubs;
import java.io.IOException;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class NYSEReducer extends Reducer<Text, IntWritable, Text, DoubleWritable> {
  DoubleWritable doubleWritable=new DoubleWritable();
  
  @Override
  public void reduce(Text key, Iterable<FloatWritable> values, Context context)
      throws IOException, InterruptedException {
	float max=0;
    for (FloatWritable value : values) { 
    	if(value.get()>max)
    	max=value.get(); 
    	
    	} 
    avgLength=(double)sum/n;
    doubleWritable.set(avgLength); 
    context.write(key, doubleWritable);

  }
}