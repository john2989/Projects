package edu.stthomas.gps.tfidf;

import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.FloatWritable;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.conf.Configuration; 
import org.apache.hadoop.io.SequenceFile.CompressionType; 
import org.apache.hadoop.io.compress.CompressionCodec; 
import org.apache.hadoop.io.compress.SnappyCodec; 

public class TfidfDriver {

  public static void main(String[] args) throws Exception {

    if (args.length != 2) {
      System.out.printf(
          "Usage: WordCount <input dir> <output dir>\n");
      System.exit(-1);
    }
    
    Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true); 
    conf.setClass("mapred.map.output.compression.codec", SnappyCodec.class, CompressionCodec.class); 
    Job job = new Job(conf);
    FileOutputFormat. setCompressOutput (job, true); 
    FileOutputFormat.setOutputCompressorClass (job,SnappyCodec.class); 
    SequenceFileOutputFormat.setOutputCompressionType (job, CompressionType. BLOCK ); 

    
    job.setJarByClass(TfidfDriver.class);
    
    job.setJobName("TFIDF");
    job.setNumReduceTasks(1);

    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));

    job.setMapperClass(TfidfMapper.class);
    job.setSortComparatorClass(FloatComparator.class);
   
    job.setOutputKeyClass(FloatWritable.class);
    job.setOutputValueClass(WordFilenameWritable.class);
     
    boolean success = job.waitForCompletion(true);
    System.exit(success ? 0 : 1);
  }
}
